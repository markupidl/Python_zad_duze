{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ed2199ed-0955-42ca-b440-f4a1f4e24007",
      "metadata": {},
      "source": [
        "Analiza języka:\n",
        "W tym dokumencie przeprowadzam analizę języka angielskiego, polskiego i niemieckiego do języka na stronach internetowych (głównie https://bulbapedia.bulbagarden.net/wiki/). Wzór to jest suma wszystkich: dicti.get(word, 0)*(1 - abs(ilosc_slow[word] - dicti.get(word, 0))), gdzie dicti to zbiór k>0 słów języka en/pl/de, a ilosc_slow to zbiór wszystkich słów na danej stronie (w głównej części), obie znormalizowane przez wspólną największą wartościowo w dicti częstotliwość słowa (odpowiednio dla każdego słownika), jeśli nie posiadają żadnego wspólnego słowa to prawdopodobieńtwo jest wyzerowane. W teście sprawdzam różne wartości k={3, 10, 100, 1000}, na stronach z wieloma słowami z wiki: https://bulbapedia.bulbagarden.net/wiki/Abomasnow_(Pok%C3%A9mon), spośród 50 randomowych stron z wiki stronę z najgorszym wynikiem funkcji: \"https://bulbapedia.bulbagarden.net/wiki/Category:Pok%C3%A9mon_by_body_size\", artykułów stron rządkowych (gov) odpowiednio Wielkiej Brytanii, Polski i Niemiec. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a59c44d-f4de-4abe-81de-eb76c48998ac",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from wordfreq import zipf_frequency, top_n_list\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "class Count_words:\n",
        "    @staticmethod\n",
        "    def count_words(strona, co1, co2):\n",
        "        dictionary = Count.count(strona, co1, co2)\n",
        "        with open(\"word-counts\" + '.json', 'w', encoding=\"utf-8\") as plik:\n",
        "            json.dump(dictionary, plik, ensure_ascii=False, indent=2)\n",
        "\n",
        "class Count:\n",
        "    @staticmethod\n",
        "    def count(strona, co1, co2):\n",
        "        response = requests.get(strona)\n",
        "        dicti = {}\n",
        "        if response.status_code == 200: \n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            text_paragraphs = soup.find(co2, class_=co1).text\n",
        "            ile = 0\n",
        "            words = re.findall(r\"\\b\\w+\\b\", text_paragraphs)\n",
        "            for i, word in enumerate(words):\n",
        "                dicti[word] = dicti.get(word, 0) + 1\n",
        "            return dicti\n",
        "        else:\n",
        "            print(\"Strona nie istnieje\")\n",
        "            return dicti\n",
        "\n",
        "def language_words_with_frequency(jezyk, ile):\n",
        "    words = top_n_list(jezyk, ile)\n",
        "    dicti = {}\n",
        "    for i, word in enumerate(words):\n",
        "        dicti[word] = zipf_frequency(word, jezyk)\n",
        "    return dicti\n",
        "\n",
        "def znormalizuj(ilosc_slow, dicti):\n",
        "    ile = 0\n",
        "    ile2 = 0\n",
        "    for word in ilosc_slow:\n",
        "        if dicti.get(word, 0) != 0:\n",
        "            if ile <  dicti.get(word, 0):\n",
        "                ile = dicti.get(word, 0)\n",
        "                ile2 = ilosc_slow[word]\n",
        "    if ile != 0:\n",
        "        for word in ilosc_slow:\n",
        "            ilosc_slow[word] = ilosc_slow[word]/ile2\n",
        "            if dicti.get(word, 0) != 0:  dicti[word] = dicti.get(word, 0)/ile\n",
        "    else:\n",
        "        for word in ilosc_slow:\n",
        "            if dicti.get(word, 0) != 0: dicti[word] = 0\n",
        "    return ilosc_slow, dicti\n",
        "\n",
        "def lang_confidence_score(ilosc_slow, dicti):\n",
        "    ilosc_slow, dicti = znormalizuj(ilosc_slow, dicti)\n",
        "    prawdopodobienstwo = 0\n",
        "    slowo = 0\n",
        "    for word in ilosc_slow:\n",
        "        if ilosc_slow[word] > slowo: slowo = ilosc_slow[word]\n",
        "    for word in ilosc_slow:\n",
        "        ilosc_slow[word] = ilosc_slow[word]/slowo\n",
        "    for word in ilosc_slow:\n",
        "        prawdopodobienstwo += dicti.get(word, 0)*(1 - abs(ilosc_slow[word] - dicti.get(word, 0)))\n",
        "    return prawdopodobienstwo\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def testy():\n",
        "    results = []\n",
        "    Count_words.count_words(\"https://bulbapedia.bulbagarden.net/wiki/Abomasnow_(Pok%C3%A9mon)\", 'mw-body-content', 'div')\n",
        "    with open(\"word-counts\" + '.json', 'r', encoding=\"utf-8\") as plik:\n",
        "        dicti_wiki_dlugi = json.load(plik)\n",
        "    Count_words.count_words(\"https://bulbapedia.bulbagarden.net/wiki/Category:Pok%C3%A9mon_by_body_size\", 'mw-body-content', 'div')\n",
        "    with open(\"word-counts\" + '.json', 'r', encoding=\"utf-8\") as plik:\n",
        "        dicti_wiki_zly_wynik = json.load(plik)\n",
        "    Count_words.count_words(\"https://www.gov.uk/government/news/greater-access-to-breakthrough-trials-for-rare-cancer-patients\", 'govuk-main-wrapper', 'main')\n",
        "    with open(\"word-counts\" + '.json', 'r', encoding=\"utf-8\") as plik:\n",
        "        dicti_nie_wiki_en = json.load(plik)\n",
        "    Count_words.count_words(\"https://www.bundesregierung.de/breg-de/service/newsletter-und-abos/bulletin/bk-regierungserklaerung-2405056\", 'bpa-container', 'div')\n",
        "    with open(\"word-counts\" + '.json', 'r', encoding=\"utf-8\") as plik:\n",
        "        dicti_nie_wiki_de = json.load(plik)\n",
        "    Count_words.count_words(\"https://www.gov.pl/web/baza-wiedzy/prawo-o-dostepnosci-cyfrowej\", \"article-area main-container\", 'div')\n",
        "    with open(\"word-counts\" + '.json', 'r', encoding=\"utf-8\") as plik:\n",
        "        dicti_nie_wiki_pl = json.load(plik)\n",
        "    k_lista = [3, 10, 100, 1000]\n",
        "    for k in k_lista:\n",
        "        #dicti_wiki_dlugi\n",
        "        score = lang_confidence_score(dicti_wiki_dlugi, language_words_with_frequency(\"en\", k))\n",
        "        results.append({\n",
        "            \"text\": \"dicti_wiki_dlugi\",\n",
        "            \"language\": \"en\",\n",
        "            \"k\": k,\n",
        "            \"score\": score\n",
        "        })\n",
        "        score = lang_confidence_score(dicti_wiki_dlugi, language_words_with_frequency(\"pl\", k))\n",
        "        results.append({\n",
        "            \"text\": \"dicti_wiki_dlugi\",\n",
        "            \"language\": \"pl\",\n",
        "            \"k\": k,\n",
        "            \"score\": score\n",
        "        })\n",
        "        score = lang_confidence_score(dicti_wiki_dlugi, language_words_with_frequency(\"de\", k))\n",
        "        results.append({\n",
        "            \"text\": \"dicti_wiki_dlugi\",\n",
        "            \"language\": \"de\",\n",
        "            \"k\": k,\n",
        "            \"score\": score\n",
        "        })\n",
        "        # dicti_wiki_zly_wynik\n",
        "        score = lang_confidence_score(dicti_wiki_zly_wynik, language_words_with_frequency(\"en\", k))\n",
        "        results.append({\n",
        "            \"text\": \"dicti_wiki_zly_wynik\",\n",
        "            \"language\": \"en\",\n",
        "            \"k\": k,\n",
        "            \"score\": score\n",
        "        })\n",
        "        score = lang_confidence_score(dicti_wiki_zly_wynik, language_words_with_frequency(\"pl\", k))\n",
        "        results.append({\n",
        "            \"text\": \"dicti_wiki_zly_wynik\",\n",
        "            \"language\": \"pl\",\n",
        "            \"k\": k,\n",
        "            \"score\": score\n",
        "        })\n",
        "        score = lang_confidence_score(dicti_wiki_zly_wynik, language_words_with_frequency(\"de\", k))\n",
        "        results.append({\n",
        "            \"text\": \"dicti_wiki_zly_wynik\",\n",
        "            \"language\": \"de\",\n",
        "            \"k\": k,\n",
        "            \"score\": score\n",
        "        })\n",
        "        # dicti_nie_wiki_en\n",
        "        score = lang_confidence_score(dicti_nie_wiki_en, language_words_with_frequency(\"en\", k))\n",
        "        results.append({\n",
        "            \"text\": \"dicti_nie_wiki_en\",\n",
        "            \"language\": \"en\",\n",
        "            \"k\": k,\n",
        "            \"score\": score\n",
        "        })\n",
        "        score = lang_confidence_score(dicti_nie_wiki_en, language_words_with_frequency(\"pl\", k))\n",
        "        results.append({\n",
        "            \"text\": \"dicti_nie_wiki_en\",\n",
        "            \"language\": \"pl\",\n",
        "            \"k\": k,\n",
        "            \"score\": score\n",
        "        })\n",
        "        score = lang_confidence_score(dicti_nie_wiki_en, language_words_with_frequency(\"de\", k))\n",
        "        results.append({\n",
        "            \"text\": \"dicti_nie_wiki_en\",\n",
        "            \"language\": \"de\",\n",
        "            \"k\": k,\n",
        "            \"score\": score\n",
        "        })\n",
        "        # dicti_nie_wiki_pl\n",
        "        score = lang_confidence_score(dicti_nie_wiki_pl, language_words_with_frequency(\"en\", k))\n",
        "        results.append({\n",
        "            \"text\": \"dicti_nie_wiki_pl\",\n",
        "            \"language\": \"en\",\n",
        "            \"k\": k,\n",
        "            \"score\": score\n",
        "        })\n",
        "        score = lang_confidence_score(dicti_nie_wiki_pl, language_words_with_frequency(\"pl\", k))\n",
        "        results.append({\n",
        "            \"text\": \"dicti_nie_wiki_pl\",\n",
        "            \"language\": \"pl\",\n",
        "            \"k\": k,\n",
        "            \"score\": score\n",
        "        })\n",
        "        score = lang_confidence_score(dicti_nie_wiki_pl, language_words_with_frequency(\"de\", k))\n",
        "        results.append({\n",
        "            \"text\": \"dicti_nie_wiki_pl\",\n",
        "            \"language\": \"de\",\n",
        "            \"k\": k,\n",
        "            \"score\": score\n",
        "        })\n",
        "        # dicti_nie_wiki_de\n",
        "        score = lang_confidence_score(dicti_nie_wiki_de, language_words_with_frequency(\"en\", k))\n",
        "        results.append({\n",
        "            \"text\": \"dicti_nie_wiki_de\",\n",
        "            \"language\": \"en\",\n",
        "            \"k\": k,\n",
        "            \"score\": score\n",
        "        })\n",
        "        score = lang_confidence_score(dicti_nie_wiki_de, language_words_with_frequency(\"pl\", k))\n",
        "        results.append({\n",
        "            \"text\": \"dicti_nie_wiki_de\",\n",
        "            \"language\": \"pl\",\n",
        "            \"k\": k,\n",
        "            \"score\": score\n",
        "        })\n",
        "        score = lang_confidence_score(dicti_nie_wiki_de, language_words_with_frequency(\"de\", k))\n",
        "        results.append({\n",
        "            \"text\": \"dicti_nie_wiki_de\",\n",
        "            \"language\": \"de\",\n",
        "            \"k\": k,\n",
        "            \"score\": score\n",
        "        })\n",
        "    \n",
        "    grouped = defaultdict(list)\n",
        "\n",
        "    for r in results:\n",
        "        key = (r[\"text\"], r[\"language\"])\n",
        "        grouped[key].append((r[\"k\"], r[\"score\"]))\n",
        "    \n",
        "    language_colors = {\n",
        "        \"en\": \"-\",\n",
        "        \"pl\": \"--\",\n",
        "        \"de\": \":\"\n",
        "    }\n",
        "\n",
        "    text_styles = {\n",
        "        \"dicti_wiki_dlugi\": \"tab:blue\",\n",
        "        \"dicti_wiki_zly_wynik\": \"tab:orange\",\n",
        "        \"dicti_nie_wiki_en\": \"tab:green\",\n",
        "        \"dicti_nie_wiki_pl\": \"tab:purple\",\n",
        "        \"dicti_nie_wiki_de\": \"tab:red\"\n",
        "    }\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    \n",
        "    for (text, lang), vals in grouped.items():\n",
        "        vals.sort()\n",
        "        ks, scores = zip(*vals)\n",
        "        plt.plot(ks, scores, color=text_styles[text], linestyle=language_colors[lang], marker=\"o\", label=f\"{text} / {lang}\")\n",
        "    \n",
        "    plt.xscale(\"log\")\n",
        "    plt.xlabel(\"k (top words)\")\n",
        "    plt.ylabel(\"confidence score\")\n",
        "    plt.title(\"Language confidence vs k\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "testy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "733eb0f2-0174-4374-8823-7fc5b5ba8b17",
      "metadata": {},
      "source": [
        "Wyniki:\n",
        "Jak widać, im większe k tym większe prawdopodobieństwo przynależenia do języka, ze względu na najprawdopodobniej małą odmianę słów w angielskim, jak widać artykuły (oprócz tego specjalnie wybranego jako złośliwego) mają bardzo dobre wyniki przynależenia (zgodnie z prawdą). Artykuł złośliwy był bardzo krótki (więcej niż 20 słów), stąd małe przynależenie słownictwa i niski wynik przynależenia, artykuł nie był bardzo trudny do znalezienia, widać spore zmiany w wartościach prawdopodobieństwa i szukając wsród krótkich artykułów znalezienie złego wyniku nie było zbyt trudne. Generalnie widać dużą poprawę przynależenia do jezyka wraz z zwiększeniem k w angielskim i polskim, ale nie niemieckim, co zaskakujące największy skok był przy k=100, tymczasem dla k=1000 nie było dużej poprawy, zarazem słowniki języka polskiego i angielskiego zaczęły mieć wspólne słowa z artykułem po niemiecku, co w pewnien sposób pogarsza wynik. Polski język ma również spory wzrost wraz z k, jednak mniejszy niż angielski, zapewne przez sporą ilość odmian słów w polskim i rozróżnialność tego samego słowa w różnych odmianach jako dwa różne słowa. Po wynikach języków po których artykuł nie został napisany widać, że język angielski ma najwięcej wspólnych słów z niemieckim i polskim, następnie polski ma sporo podobieństw z niemieckim i angielskim, najmniejsze podobieństwo ma język niemiecki. Jak widać dobór języków ma spore znaczenie, ze względu na podobne słowa między językami, oryginalne litery w słownictwie, odmiany słów w języku. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.13 (XPython)",
      "language": "python",
      "name": "xpython"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
